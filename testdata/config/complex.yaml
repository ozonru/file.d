settings:
  log_level: debug
  metrics_port: 4000

pipelines:
  docker-logs:                    # pipeline for docker logs
    input: docker                 # get logs from docker containers
    processors:
    - add_k8s_meta                # add kubernetes meta info
    - discard_unnecessary_events: # don't process some logs defined by OR regexp operation
      match_fields:               # filter will be used when any of regexp matches
        kubernetes_namespace_name: /kube-system|ozon-system|ingress-nginx/
        kubernetes_container_name: /logstash|filebeat|cherkizon-event-manager|graylog|fluentd/
      match_or: true
    - normalize                   # do some modifications
    - go_multiline                # join go panics
    - dotnet_multiline            # join dotnet panics
    - setup_kafka_topic:          # set default kafka topic
      args:                       # pass some values to filter
        topic: prod-k8s-filebeat-logs
    - setup_kafka_topic:          # set kafka topic for travel namespace
      match_fields:               # filter will be used when all fields matches
        kubernetes_namespace_name: travel
      args:                       # pass some values to filter
        topic: prod-k8s-filebeat-travel-logs
    - setup_kafka_topic:          # set kafka topic for lms service
      match_fields:               # filter will be used when all fields matches
        kubernetes_label_app: lms-go-service-sorter-system-vanderlande-driver
      args:                       # pass some values to filter
        topic: prod-k8s-filebeat-vanderlande-driver
    - cleanup_fields              # keep only useful fields
    output: kafka                 # send logs to kafka

  trickster_slow_log:             # pipeline for trickster slow logs
    input: trickster_slow_log
    processors:
    - add_host_meta
    output: kafka

processors:
  add_k8s_meta:                   # define `add_k8s_meta` filter
    type: k8s_meta                # `add_k8s_meta` filter is a `k8s_meta` plugin

  discard_unnecessary:            # define `discard_unnecessary` filter
    type: discard                 # `discard_unnecessary` filter is a `discard` plugin

  setup_kafka_topic:              # define `setup_kafka_topic` filter
    type: modify_fields           # `setup_kafka_topic` filter is a `modify` plugin
    tmp.kafka_topic: $args.topic  # place passed argument into field `~topic`. all fields started with `~` are temporary and won't be in final json.

  normalize:                      # define `normalize` filter
    type: modify_fields
    some_field: $event.kubernetes.namespace                       # copy field `kubernetes.namespace` into `some_field`
    pipeline: docker                                              # add field with const value
    complex: this is $event.kubernetes.namespace                  # do some templating
    kubernetes_host: $event.host.name,drop                        # rename field `host.name` to `kubernetes_host`. `drop` at the end means that field `host.name` should be deleted after templating.
    kubernetes_namespace_name: $event.kubernetes.namespace,drop   # rename field `kubernetes.namespace` to `kubernetes_namespace_name`

  cleanup_fields:
    type: filter_fields
    mode: keep|delete
    fields: [beat, offset, kubernetes.container, kubernetes.replicaset] # delete `beat`, `offset` and  some other fields

inputs:
  docker-logs:                    # define `docker` input
    type: docker
    path: "/var/lib/docker/containers"
    offset_file: "/data/offsets.json"

outputs:
  kafka:                          # define `kafka` output
    type: kafka
    brokers: sreinfrakafka1198z20.h.o3.ru:9092,sreinfrakafka1199z20.h.o3.ru:9092
    batch_bytes: 10M
    topic: $event.tmp.kafka_topic # use `~kafka_topic` field as topic for kafka